executor: KubernetesExecutor

images:
  airflow:
    repository: daappoc25.azurecr.io/airflow
    tag: custom
    pullPolicy: IfNotPresent

airflow:
  config:
    AIRFLOW__OPENMETADATA__API_VERSION: "v1"
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
    AIRFLOW__METRICS__METRICS_USE_PATTERN_MATCH: "True"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow_user:airflow_password@airflow-postgresql.daap-dev.svc.cluster.local:5432/airflow_db"
    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 10
    AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: 20

  env:
    - name: AIRFLOW__OPENMETADATA__HOST
      value: "http://128.85.217.88:8585"
    - name: AIRFLOW__OPENMETADATA__API_VERSION
      value: "v1"
    - name: AIRFLOW__API__AUTH_BACKENDS
      value: "airflow.api.auth.backend.basic_auth"
    - name: AIRFLOW__AUTH__AUTH_MANAGER
      value: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"

  extraVolumeMounts:
    - name: airflow-plugins
      mountPath: /opt/airflow/plugins

  webserver:
    service:
      type: LoadBalancer
      port: 8080
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    readinessProbe:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    startupProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30  
      periodSeconds: 10
      failureThreshold: 10     
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1

  scheduler:
    replicas: 1
    livenessProbe:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    readinessProbe:
      enabled: true
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    startupProbe:
      enabled: true
      exec:
        command:
          - sh
          - -c
          - |
            CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
            airflow jobs check --job-type SchedulerJob --local
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 30
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1
        memory: 1Gi

  workers:
    replicas: 1
    persistence:
      enabled: true
      size: 500Mi
      accessModes:
        - ReadWriteOnce
      storageClassName: managed-csi
    resources:
      requests:
        memory: 128Mi
        cpu: 100m
      limits:
        memory: 256Mi
        cpu: 200m

  triggerer:
    persistence:
      enabled: false
      size: 500Mi
      accessModes:
        - ReadWriteOnce
      storageClassName: managed-csi
    livenessProbe:
      enabled: false
    readinessProbe:
      enabled: false
    startupProbe:
      enabled: false

  dags:
    persistence:
      enabled: false
      size: 500Mi
      accessModes:
        - ReadWriteOnce
      storageClassName: managed-csi

  logs:
    persistence:
      enabled: true
      size: 500Mi
      accessModes:
        - ReadWriteOnce
      storageClassName: managed-csi

  createUserJob:
    useHelmHooks: true

  migrateDatabaseJob:
    useHelmHooks: true

postgresql:
  enabled: true
  auth:
    username: airflow_user
    password: airflow_password
    database: airflow_db
  primary:
    persistence:
      enabled: true
      size: 2Gi
      storageClass: managed-csi
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 512Mi
        cpu: 500m
